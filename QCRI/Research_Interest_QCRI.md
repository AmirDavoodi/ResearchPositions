## Research Interests — Multimodal VLMs + Knowledge Graphs for Crisis Response

I am interested in building trustworthy multimodal foundation models (vision–language–action) that integrate structured knowledge for crisis and disaster management. My current research focuses on:

- Agentic AI for Knowledge Graph Construction (KGC): dual-learning validation (graph↔text) to reduce hallucinations and improve reliability in automatically built crisis/domain KGs.
- GraphRAG-style retrieval and grounding: aligning VLMs/VLAs with crisis knowledge graphs (entities, events, locations, resources), enabling explainable and auditable answers.
- Temporal/event graphs from heterogeneous streams: fusing social media, satellite/aerial imagery, and sensor/time-series into dynamic graphs for situational awareness.
- Robustness, uncertainty, and XAI: perturbation-based explanation for graph features; calibrated uncertainty for triage and prioritization.

Example directions at QCRI:
- KG-augmented VLM grounding for geolocated crisis imagery and reports
- Event graph construction from social media streams with dual validation
- Multimodal GraphRAG for decision support (needs/resources matching, route planning)
- Benchmarks: reliability, faithfulness, and human factors for crisis AI

Recent work: ASD-GraphNet (fMRI graph learning for autism diagnosis, Computers in Biology and Medicine, 2025; doi:10.1016/j.compbiomed.2025.110723). Under review: HIDE-KG — dual-learning KGC with LLMs for reliable graph construction.
