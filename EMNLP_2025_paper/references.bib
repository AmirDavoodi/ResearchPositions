@book{dualLearning2020,
  title     = {Dual Learning},
  author    = {Tao Qin},
  year      = {2020},
  publisher = {Springer},
  address   = {Singapore},
  isbn      = {978-981-15-8884-6},
  doi       = {10.1007/978-981-15-8884-6},
  url       = {https://link.springer.com/book/10.1007/978-981-15-8884-6}
}

@misc{IZeroShot2023,
  title         = {Iterative Zero-Shot LLM Prompting for Knowledge Graph Construction},
  author        = {Salvatore Carta and Alessandro Giuliani and Leonardo Piano and Alessandro Sebastian Podda and Livio Pompianu and Sandro Gabriele Tiddia},
  year          = {2023},
  eprint        = {2307.01128},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2307.01128}
}

@inproceedings{GoG2024,
  author    = {Xu, Yao and He, Shizhu and Chen, Jiabei and Wang, Zihao and Song, Yangqiu and Tong, Hanghang and Liu, Guang and Zhao, Jun and Liu, Kang},
  year      = {2024},
  title     = {Generate-on-Graph: Treat LLM as both Agent and KG for Incomplete Knowledge Graph Question Answering},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages     = {18410--18430},
  doi       = {10.18653/v1/2024.emnlp-main.1023}
}

@misc{huang2024llmsgoodgraphjudger,
  title         = {Can LLMs be Good Graph Judger for Knowledge Graph Construction?},
  author        = {Haoyu Huang and Chong Chen and Conghui He and Yang Li and Jiawei Jiang and Wentao Zhang},
  year          = {2024},
  eprint        = {2411.17388},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2411.17388}
}

@inproceedings{zeroShotKGBuilder2024,
  author    = {Carta, Salvatore and Giuliani, Alessandro and Manca, Marco Manolo and Piano, Leonardo and Tiddia, Sandro Gabriele},
  title     = {Towards Zero-shot Knowledge Graph building: Automated Schema Inference},
  year      = {2024},
  isbn      = {9798400704666},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3631700.3665234},
  doi       = {10.1145/3631700.3665234},
  abstract  = {In the current Digital Transformation scenario, Knowledge Graphs are essential for comprehending, representing, and exploiting complex information in a structured form. The main paradigm in automatically generating proper Knowledge Graphs relies on predefined schemas or ontologies. Such schemas are typically manually constructed, requiring an intensive human effort, and are often sensitive to information loss due to negligence, incomplete analysis, or human subjectivity or inclination. Limiting human bias and the resulting information loss in creating proper Knowledge Graphs is paramount, particularly for user modeling in various sectors, such as education or healthcare. To this end, we propose a novel approach to automatically generating a proper entity schema. The devised methodology combines the language understanding capabilities of LLM with classical machine learning methods such as clustering to properly build an entity schema from a set of documents. This solution eliminates the need for human intervention and fosters a more efficient and comprehensive knowledge representation. The assessment of our proposal concerns adopting a state-of-the-art entity extraction model (UniNER) to estimate the relevance of the extracted entities based on the generated schema. Results confirm the potential of our approach, as we observed a negligible difference between the topic similarity score obtained with the ground truth and with the automatically generated schema (less than 1\% on average on three different datasets). Such an outcome confirms that the proposed approach may be valuable in automatically creating an entity schema from a set of documents.},
  booktitle = {Adjunct Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization},
  pages     = {467--473},
  numpages  = {7},
  keywords  = {Large Language Models, Named Entity Recognition, Ontology Learning},
  location  = {Cagliari, Italy},
  series    = {UMAP Adjunct '24}
}

@article{surveyKGConstruction2023,
  author     = {Zhong, Lingfeng and Wu, Jia and Li, Qian and Peng, Hao and Wu, Xindong},
  title      = {A Comprehensive Survey on Automatic Knowledge Graph Construction},
  year       = {2023},
  issue_date = {April 2024},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {56},
  number     = {4},
  issn       = {0360-0300},
  url        = {https://doi.org/10.1145/3618295},
  doi        = {10.1145/3618295},
  abstract   = {Automatic knowledge graph construction aims at manufacturing structured human knowledge. To this end, much effort has historically been spent extracting informative fact patterns from different data sources. However, more recently, research interest has shifted to acquiring conceptualized structured knowledge beyond informative data. In addition, researchers have also been exploring new ways of handling sophisticated construction tasks in diversified scenarios. Thus, there is a demand for a systematic review of paradigms to organize knowledge structures beyond data-level mentions. To meet this demand, we comprehensively survey more than 300 methods to summarize the latest developments in knowledge graph construction. A knowledge graph is built in three steps: knowledge acquisition, knowledge refinement, and knowledge evolution. The processes of knowledge acquisition are reviewed in detail, including obtaining entities with fine-grained types and their conceptual linkages to knowledge graphs; resolving coreferences; and extracting entity relationships in complex scenarios. The survey covers models for knowledge refinement, including knowledge graph completion, and knowledge fusion. Methods to handle knowledge evolution are also systematically presented, including condition knowledge acquisition, condition knowledge graph completion, and knowledge dynamic. We present the paradigms to compare the distinction among these methods along the axis of the data environment, motivation, and architecture. Additionally, we also provide briefs on accessible resources that can help readers to develop practical knowledge graph systems. The survey concludes with discussions on the challenges and possible directions for future exploration.},
  journal    = {ACM Comput. Surv.},
  month      = nov,
  articleno  = {94},
  numpages   = {62},
  keywords   = {logic reasoning, knowledge fusion, knowledge graph completion, information extraction, deep learning, Knowledge graph}
}

@article{KGLLMHallucinations2025,
  title    = {Knowledge Graphs, Large Language Models, and Hallucinations: An NLP Perspective},
  journal  = {Journal of Web Semantics},
  volume   = {85},
  pages    = {100844},
  year     = {2025},
  issn     = {1570-8268},
  doi      = {https://doi.org/10.1016/j.websem.2024.100844},
  url      = {https://www.sciencedirect.com/science/article/pii/S1570826824000301},
  author   = {Ernests Lavrinovics and Russa Biswas and Johannes Bjerva and Katja Hose},
  keywords = {LLM, Factuality, Knowledge Graphs, Hallucinations}
}

@inproceedings{ThinkOnGraph2024,
  title     = {Think-on-Graph: Deep and Responsible Reasoning of Large Language Model with Knowledge Graph},
  author    = {Zhang, Yuxuan and Wang, Yuxuan},
  booktitle = {Proceedings of the 2024 International Conference on Artificial Intelligence},
  year      = {2024},
  url       = {https://deepai.org/publication/think-on-graph-deep-and-responsible-reasoning-of-large-language-model-with-knowledge-graph}
}

@inproceedings{luan2018multi,
  title     = {Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction},
  author    = {Luan, Yi and He, Luheng and Ostendorf, Mari and Hajishirzi, Hannaneh},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages     = {3219--3232},
  year      = {2018},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/D18-1360/},
  doi       = {10.18653/v1/D18-1360}
}

@inproceedings{zavarella2024a,
  title     = {A Few-Shot Approach for Relation Extraction Domain Adaptation using Large Language Models},
  author    = {Vanni Zavarella and Juan Carlos Gamero and Sergio Consoli},
  booktitle = {Workshop on Deep Learning and Large Language Models for Knowledge Graphs},
  year      = {2024},
  url       = {https://openreview.net/forum?id=rBUbEKOECY}
}

@inproceedings{Eberts_Markus_2020,
  title     = {Span-Based Joint Entity and Relation Extraction with Transformer Pre-Training},
  booktitle = {ECAI 2020},
  publisher = {IOS Press},
  author    = {Eberts, Markus and Ulges, Adrian},
  year      = {2020},
  doi       = {10.3233/FAIA200321},
  url       = {http://dx.doi.org/10.3233/FAIA200321},
  pages     = {2003--2010}
}

@misc{peeters2024entitymatchingusinglarge,
      title={Entity Matching using Large Language Models}, 
      author={Ralph Peeters and Aaron Steiner and Christian Bizer},
      year={2024},
      eprint={2310.11244},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.11244}, 
}
